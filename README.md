# Toxic-Comment-Classification
Code for creating a model which can detect different types of toxicity in the comments

## Problem Description
Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments.

## Problem Statement
The objective is to build a model which can detect different types of toxicity better than the current models. There is only one predictor variable here in the dataset i.e. "comment_text" respective to which we have 6 target variables describing the type of toxicity:

● toxic
● severe_toxic
● obscene
● threat
● insult
● identity_hate

## File descriptions
##### train.csv:
the training set, contains comments with their binary labels.
##### test.csv:
the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.
##### sample_submission.csv:
a sample submission file in the correct format.

### I have included the project report. 
### Soon I'll be uploading the HTML version for the same.
